{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lygitdata/aml_project/blob/main/project2/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGfw7IukYeYW"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yD-hZ8l9Ybmd"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pywt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from imblearn import over_sampling, pipeline\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import ensemble, model_selection, preprocessing, feature_selection, svm, linear_model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Change this path to the folder that has your data\n",
    "fpath = \"aml_p2/data_2/\"\n",
    "RANDOM_STATE = 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k2tC0dNYhL9"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SfcAO82VYjRF"
   },
   "outputs": [],
   "source": [
    "X_train = np.load(f\"{fpath}X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(f\"{fpath}X_test.npy\", allow_pickle=True)\n",
    "y_train = np.load(f\"{fpath}y_train.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx1UdoBtZAtY"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.4221947194719472, 1: 2.8876975169300225, 2: 0.867876526458616, 3: 7.525}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Class labels\n",
    "class_labels = np.unique(y_train)  # Assuming one-hot encoded y_train_updated\n",
    "\n",
    "# Compute weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=class_labels,\n",
    "    y=y_train  # Convert one-hot to class indices\n",
    ")\n",
    "\n",
    "# Convert to dictionary format\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kjsm6LzIZFYp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (5117, 398) \n",
      "X_test shape (3411, 398)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"X_train shape: \",\n",
    "    X_train.shape,\n",
    "    \"\\nX_test shape\",\n",
    "    X_test.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.4221947194719472, 1: 2.8876975169300225, 2: 0.867876526458616, 3: 7.525}\n",
      "X_train shape:  (5117, 398) \n",
      "X_test shape (3411, 398)\n"
     ]
    }
   ],
   "source": [
    "# Replace None with np.nan\n",
    "X_train = np.where(X_train == None, np.nan, X_train)\n",
    "X_test = np.where(X_test == None, np.nan, X_test)\n",
    "\n",
    "# Check for infinity or NaN values in X_train and replace them with a large finite number or the mean of the column\n",
    "X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train), posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "X_test = np.nan_to_num(X_test, nan=np.nanmean(X_test), posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "print(\"X_train shape: \", X_train.shape, \"\\nX_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in X_train: {<class 'numpy.float64'>}\n",
      "Data types in X_test: {<class 'numpy.float64'>}\n",
      "Largest value in X_train: 1.7976931348623157e+308\n",
      "Largest value in X_test: 18054887424.0\n",
      "Indices of the 20 largest values in X_train: (array([2949, 2370, 3078, 1380, 1520, 1579, 2626,  415, 4574,  715, 3098,\n",
      "       2135, 1832, 4341, 1998, 4945,  840, 2369, 1277, 4461], dtype=int64), array([322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322,\n",
      "       322, 322, 322, 322, 322, 374, 374], dtype=int64))\n",
      "Indices of the 20 largest values in X_test: (array([1276, 1958,  225, 2934,  337,  619, 2937, 1207, 1670, 2865, 2600,\n",
      "       2301, 2809, 2764, 2213,  980,  118,  612,  164, 2872], dtype=int64), array([322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322,\n",
      "       322, 322, 322, 322, 322, 322, 322], dtype=int64))\n",
      "20 largest values in X_train: [1.11059336e+08 1.11156598e+08 1.11949207e+08 1.14619451e+08\n",
      " 1.30929591e+08 1.31321655e+08 1.32933969e+08 1.36367236e+08\n",
      " 1.37826705e+08 1.38779035e+08 1.86412942e+08 1.98382893e+08\n",
      " 2.25824941e+08 2.29053373e+08 2.29117666e+08 2.45805835e+08\n",
      " 2.55803079e+08 2.72695473e+08 4.55001495e+08 5.50628570e+08]\n",
      "20 largest values in X_test: [9.45241173e+07 1.03401226e+08 1.05522481e+08 1.07293014e+08\n",
      " 1.09969740e+08 1.12210168e+08 1.14590152e+08 1.15295114e+08\n",
      " 1.17881795e+08 1.24708421e+08 1.25159952e+08 1.42835367e+08\n",
      " 1.43360749e+08 1.54002512e+08 1.58700605e+08 1.62361573e+08\n",
      " 1.81207050e+08 2.08822822e+08 2.27975484e+08 2.29425759e+08]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# Ensure no infinity or too large values\n",
    "X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train), posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "X_test = np.nan_to_num(X_test, nan=np.nanmean(X_test), posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "\n",
    "print(\"Data types in X_train:\", set(map(type, X_train.flatten())))\n",
    "print(\"Data types in X_test:\", set(map(type, X_test.flatten())))\n",
    "print(\"Largest value in X_train:\", np.max(X_train))\n",
    "print(\"Largest value in X_test:\", np.max(X_test))\n",
    "\n",
    "# Find the indices of the largest values\n",
    "largest_value_indices_train = np.unravel_index(np.argsort(X_train, axis=None)[-20:], X_train.shape)\n",
    "largest_value_indices_test = np.unravel_index(np.argsort(X_test, axis=None)[-20:], X_test.shape)\n",
    "\n",
    "print(\"Indices of the 20 largest values in X_train:\", largest_value_indices_train)\n",
    "print(\"Indices of the 20 largest values in X_test:\", largest_value_indices_test)\n",
    "\n",
    "# Discard columns with the largest values\n",
    "columns_to_discard_train = np.unique(largest_value_indices_train[1])\n",
    "columns_to_discard_test = np.unique(largest_value_indices_test[1])\n",
    "\n",
    "X_train = np.delete(X_train, columns_to_discard_train, axis=1)\n",
    "X_test = np.delete(X_test, columns_to_discard_test, axis=1)\n",
    "\n",
    "# print(\"X_train shape after discarding columns:\", X_train.shape)\n",
    "# print(\"X_test shape after discarding columns:\", X_test.shape)\n",
    "# Check the 20 largest values in X_train and X_test\n",
    "largest_values_train = np.sort(X_train.flatten())[-20:]\n",
    "largest_values_test = np.sort(X_test.flatten())[-20:]\n",
    "\n",
    "print(\"20 largest values in X_train:\", largest_values_train)\n",
    "print(\"20 largest values in X_test:\", largest_values_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhB8VtT8Y_25"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdENCqCBdj08"
   },
   "outputs": [],
   "source": [
    "model = pipeline.make_pipeline(\n",
    "    # over_sampling.RandomOverSampler(random_state=RANDOM_STATE),\n",
    "    preprocessing.StandardScaler(),\n",
    "    # Select K best features\n",
    "    # feature_selection.SelectKBest(k=100),\n",
    "    ensemble.StackingClassifier(\n",
    "        estimators = [\n",
    "            (\"hgb\", ensemble.HistGradientBoostingClassifier(l2_regularization=0.15, max_iter=400, random_state=0)),\n",
    "            (\"xgb\", XGBClassifier(n_estimators=2000, learning_rate=0.11, max_depth=16, alphha=0.2, verbosity=0, random_state=0)),\n",
    "            (\"lgb\", lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.11, num_leaves=16, num_threads=128, verbose=0, random_state=0)),\n",
    "        ],\n",
    "        final_estimator=linear_model.RidgeClassifierCV()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ifRBoTXAZT4D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score:  0.794018441769544 \n",
      "Std. F1 score:  0.009525005556320507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model_selection.cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=6,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Mean F1 score: \", score.mean(), \"\\nStd. F1 score: \", score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-v_QaA9dfLs"
   },
   "source": [
    "# Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ujHT0aqoan6e"
   },
   "outputs": [],
   "source": [
    "def create_submission(model, X_train, y_train, X_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    pred = np.vstack((np.arange(X_test.shape[0]), pred)).T\n",
    "    np.savetxt(\"submission.csv\", pred, delimiter=\",\", header=\"id,y\", comments=\"\")\n",
    "\n",
    "\n",
    "create_submission(model, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFswewg8o1/mo3p6VvvSha",
   "include_colab_link": true,
   "mount_file_id": "12CBSzY_EGcvmAOVBXFcGaI9jCbhUpZFj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
