{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import biosppy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_raw shape:  (5117,) \n",
      "y_train shape (5117,) \n",
      "X_test shape (3411,)\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    X_train = pd.read_csv(train_path, index_col=\"id\")\n",
    "    y_train = X_train.iloc[:, 0]\n",
    "    X_train = X_train.iloc[:, 1:]\n",
    "    X_test = pd.read_csv(test_path, index_col=\"id\")\n",
    "    return transform_data(X_train), y_train.values, transform_data(X_test)\n",
    "\n",
    "def transform_data(df):\n",
    "    return np.array([row.dropna().to_numpy(dtype='float32') for _, row in df.iterrows()], dtype=object)\n",
    "\n",
    "X_train_raw, y_train_raw, X_test_raw = load_data(\n",
    "    train_path = \"train.csv\",\n",
    "    test_path = \"test.csv\"\n",
    ")\n",
    "print(\n",
    "    \"X_train_raw shape: \",\n",
    "    X_train_raw.shape,\n",
    "    \"\\ny_train shape\",\n",
    "    y_train_raw.shape,\n",
    "    \"\\nX_test shape\",\n",
    "    X_test_raw.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosppy.signals import ecg\n",
    "\n",
    "def preprocess_ecg_with_rpeaks(ecg_signals, sampling_rate=300, window_size=300):\n",
    "    processed_signals = []\n",
    "    segment_counts = []  # To keep track of the number of segments per signal\n",
    "    \n",
    "    for signal in ecg_signals:\n",
    "        # Detect R-peaks using BioSPPy\n",
    "        out = ecg.ecg(signal=signal, sampling_rate=sampling_rate, show=False)\n",
    "        rpeaks = out['rpeaks']\n",
    "        \n",
    "        signal_segments = []\n",
    "        \n",
    "        # Extract windows around R-peaks\n",
    "        for rpeak in rpeaks:\n",
    "            start = max(0, rpeak - window_size // 2)\n",
    "            end = min(len(signal), rpeak + window_size // 2)\n",
    "            window = signal[start:end]\n",
    "            \n",
    "            # Pad or truncate to window size\n",
    "            if len(window) < window_size:\n",
    "                window = np.pad(window, (0, window_size - len(window)), 'constant')\n",
    "            else:\n",
    "                window = window[:window_size]\n",
    "            \n",
    "            signal_segments.append(window)\n",
    "        \n",
    "        # Add the segments for this signal\n",
    "        processed_signals.extend(signal_segments)\n",
    "        segment_counts.append(len(signal_segments))  # Record the number of segments\n",
    "    \n",
    "    return np.array(processed_signals), segment_counts\n",
    "\n",
    "# Preprocess data\n",
    "X_train_segments, segment_counts_train = preprocess_ecg_with_rpeaks(X_train_raw)\n",
    "X_test_segments, segment_counts_test = preprocess_ecg_with_rpeaks(X_test_raw)\n",
    "\n",
    "np.save(\"X_train_segments.npy\", X_train_segments)\n",
    "np.save(\"segment_counts_train.npy\", segment_counts_train)\n",
    "np.save(\"X_test_segments.npy\", X_test_segments)\n",
    "np.save(\"segment_counts_test.npy\", segment_counts_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_segments = np.load(\"X_train_segments.npy\")\n",
    "segment_counts_train = np.load(\"segment_counts_train.npy\")\n",
    "X_test_segments = np.load(\"X_test_segments.npy\")\n",
    "segment_counts_test = np.load(\"segment_counts_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated X_train shape: (5117, 300, 1)\n",
      "y_train shape: (5117,)\n"
     ]
    }
   ],
   "source": [
    "# # Aggreate segments into a single feature vector per signal\n",
    "\n",
    "# def aggregate_segments(X_segments, segment_counts):\n",
    "#     aggregated_features = []\n",
    "#     start = 0\n",
    "#     for count in segment_counts:\n",
    "#         # Extract segments for the current signal\n",
    "#         signal_segments = X_segments[start:start+count]\n",
    "#         # Aggregate features (mean across all segments for simplicity)\n",
    "#         aggregated_features.append(np.mean(signal_segments, axis=0))  # Example: Mean\n",
    "#         start += count\n",
    "#     return np.array(aggregated_features)\n",
    "\n",
    "# # Aggregate training and test data\n",
    "# X_train_aggregated = aggregate_segments(X_train_segments, segment_counts_train)\n",
    "# X_test_aggregated = aggregate_segments(X_test_segments, segment_counts_test)\n",
    "\n",
    "# # Add channel dimension for CNNs\n",
    "# X_train_aggregated = X_train_aggregated[..., np.newaxis]\n",
    "# X_test_aggregated = X_test_aggregated[..., np.newaxis]\n",
    "\n",
    "# np.save(\"X_train_aggregated.npy\", X_train_aggregated)\n",
    "# np.save(\"X_test_aggregated.npy\", X_test_aggregated)\n",
    "\n",
    "# print(\"Aggregated X_train shape:\", X_train_aggregated.shape)\n",
    "# print(\"y_train shape:\", y_train_raw.shape)  # Should match now\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_segments shape: (190030, 300)\n",
      "segment_labels_train shape: (190030,)\n",
      "X_test_segments shape: (126442, 300)\n",
      "segment_labels_test shape: (126442,)\n"
     ]
    }
   ],
   "source": [
    "def get_segment_labels(segment_counts):\n",
    "    \"\"\"\n",
    "    Generate segment labels for each segment based on segment counts.\n",
    "\n",
    "    Args:\n",
    "    - segment_counts: List or array where each value represents the number of segments per signal.\n",
    "\n",
    "    Returns:\n",
    "    - segment_labels: Array of shape (total_segments,), containing the signal index for each segment.\n",
    "    \"\"\"\n",
    "    segment_labels = []\n",
    "    for signal_idx, count in enumerate(segment_counts):\n",
    "        # Add the signal index `count` times\n",
    "        segment_labels.extend([signal_idx] * count)\n",
    "    return np.array(segment_labels)\n",
    "\n",
    "# Generate segment_labels_train using segment_counts_train\n",
    "segment_labels_train = get_segment_labels(segment_counts_train)\n",
    "segment_labels_test = get_segment_labels(segment_counts_test)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_train_segments shape:\", X_train_segments.shape)  # Should match the number of segments\n",
    "print(\"segment_labels_train shape:\", segment_labels_train.shape)  # Should match num_segments\n",
    "print(\"X_test_segments shape:\", X_test_segments.shape)  # Should match the number of segments\n",
    "print(\"segment_labels_test shape:\", segment_labels_test.shape)  # Should match num_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded X_train shape: (190030, 300)\n",
      "Expanded y_train shape: (190030,)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate y_train for each segment based on segment_labels_train\n",
    "expanded_y_train = y_train_raw[segment_labels_train]\n",
    "\n",
    "# Check shapes\n",
    "print(\"Expanded X_train shape:\", X_train_segments.shape)  # Example: (190030, 300)\n",
    "print(\"Expanded y_train shape:\", expanded_y_train.shape)  # Example: (190030, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_segments shape: (190030, 300, 1, 1)\n",
      "X_test_segments shape: (126442, 300, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Add channel dimension to X_train and X_test\n",
    "# X_train_segments = X_train_segments[..., np.newaxis]  # Shape: (190030, 300, 1)\n",
    "# X_test_segments = X_test_segments[..., np.newaxis]    # Shape: (num_test_segments, 300, 1)\n",
    "\n",
    "# print(\"X_train_segments shape:\", X_train_segments.shape)\n",
    "# print(\"X_test_segments shape:\", X_test_segments.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_segments = X_test_segments.reshape(X_test_segments.shape[0], X_test_segments.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_segments.npy\", X_train_segments)\n",
    "np.save(\"segment_labels_train.npy\", segment_labels_train)\n",
    "np.save(\"X_test_segments.npy\", X_test_segments)\n",
    "np.save(\"segment_labels_test.npy\", segment_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load X_train_rpeak.npy and X_test_rpeak.npy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train_segments.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_segments.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Load X_train_rpeak.npy and X_test_rpeak.npy\n",
    "\n",
    "X_train = np.load(\"X_train_segments.npy\")\n",
    "X_test = np.load(\"X_test_segments.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (190030, 300, 1) \n",
      "X_test shape (126442, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"X_train shape: \",\n",
    "    X_train.shape,\n",
    "    \"\\nX_test shape\",\n",
    "    X_test.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 2D: (n_samples * sequence_length, 1)\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "# Reshape back to 3D: (n_samples, sequence_length, 1)\n",
    "X_train = X_train_scaled.reshape(X_train.shape)\n",
    "\n",
    "# Repeat for X_test\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "X_test = X_test_scaled.reshape(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(expanded_y_train.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up signals into windows of size 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     20\u001b[0m step_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m---> 22\u001b[0m X_train_split \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mconcatenate([split_into_windows(signal, window_size, step_size) \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m X_train_combined])\n\u001b[0;32m     23\u001b[0m y_train_split \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(y_train_updated, X_train_split\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m y_train_updated\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m X_test_split \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([split_into_windows(signal, window_size, step_size) \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m X_test_combined])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def split_into_windows(signal, window_size=500, step_size=1000):\n",
    "    \"\"\"\n",
    "    Split a signal into overlapping or non-overlapping windows.\n",
    "    \n",
    "    Args:\n",
    "    - signal: 1D array representing the signal.\n",
    "    - window_size: Length of each window.\n",
    "    - step_size: Step size between consecutive windows.\n",
    "    \n",
    "    Returns:\n",
    "    - windows: List of 1D arrays, each of length window_size.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for start in range(0, len(signal) - window_size + 1, step_size):\n",
    "        windows.append(signal[start:start + window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# Apply to training and test data\n",
    "window_size = 1000\n",
    "step_size = 1000\n",
    "\n",
    "X_train_split = np.concatenate([split_into_windows(signal, window_size, step_size) for signal in X_train_combined])\n",
    "y_train_split = np.repeat(y_train_updated, X_train_split.shape[0] // y_train_updated.shape[0], axis=0)\n",
    "\n",
    "X_test_split = np.concatenate([split_into_windows(signal, window_size, step_size) for signal in X_test_combined])\n",
    "\n",
    "print(\"New X_train_split shape:\", X_train_split.shape)  # Example: (n_samples, window_size, 2)\n",
    "print(\"New y_train_split shape:\", y_train_split.shape)\n",
    "print(\"New X_test_split shape:\", X_test_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_raw),\n",
    "    y=y_train_raw\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_22 (Conv1D)          (None, 291, 128)          1408      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 291, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 282, 128)          163968    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 282, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 141, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 141, 128)          0         \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (None, 132, 128)          163968    \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 132, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (None, 123, 128)          163968    \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 123, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPoolin  (None, 61, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 61, 128)           0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 7808)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               3998208   \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,495,620\n",
      "Trainable params: 4,494,596\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # First Convolution Block\n",
    "    Conv1D(filters=128, kernel_size=10, activation='relu', input_shape=(300, 1)),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=128, kernel_size=10, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Second Convolution Block\n",
    "    Conv1D(filters=128, kernel_size=10, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=128, kernel_size=10, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Flatten and Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(4, activation='softmax')  # Output layer for 4 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (190030, 300, 1)\n",
      "y_train shape: (190030, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "297/297 [==============================] - 67s 221ms/step - loss: 0.9012 - accuracy: 0.6335 - val_loss: 1.4802 - val_accuracy: 0.2707 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "297/297 [==============================] - 84s 284ms/step - loss: 0.7402 - accuracy: 0.6871 - val_loss: 0.8806 - val_accuracy: 0.6414 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "297/297 [==============================] - 104s 351ms/step - loss: 0.6982 - accuracy: 0.7078 - val_loss: 0.8547 - val_accuracy: 0.6487 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "297/297 [==============================] - 72s 243ms/step - loss: 0.6649 - accuracy: 0.7236 - val_loss: 0.8770 - val_accuracy: 0.6409 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "297/297 [==============================] - 68s 230ms/step - loss: 0.6343 - accuracy: 0.7378 - val_loss: 0.8761 - val_accuracy: 0.6451 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "297/297 [==============================] - 75s 252ms/step - loss: 0.6065 - accuracy: 0.7511 - val_loss: 0.8791 - val_accuracy: 0.6474 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "297/297 [==============================] - 95s 319ms/step - loss: 0.5815 - accuracy: 0.7624 - val_loss: 0.8891 - val_accuracy: 0.6512 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "297/297 [==============================] - 85s 285ms/step - loss: 0.5542 - accuracy: 0.7756 - val_loss: 0.9285 - val_accuracy: 0.6478 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "297/297 [==============================] - 98s 329ms/step - loss: 0.5182 - accuracy: 0.7913 - val_loss: 0.9315 - val_accuracy: 0.6473 - lr: 5.0000e-05\n",
      "Epoch 10/100\n",
      "297/297 [==============================] - 137s 462ms/step - loss: 0.5013 - accuracy: 0.7998 - val_loss: 0.9365 - val_accuracy: 0.6470 - lr: 5.0000e-05\n",
      "Epoch 11/100\n",
      "297/297 [==============================] - 93s 314ms/step - loss: 0.4866 - accuracy: 0.8051 - val_loss: 0.9450 - val_accuracy: 0.6463 - lr: 5.0000e-05\n",
      "Epoch 12/100\n",
      "297/297 [==============================] - 74s 249ms/step - loss: 0.4737 - accuracy: 0.8110 - val_loss: 0.9710 - val_accuracy: 0.6421 - lr: 5.0000e-05\n",
      "Epoch 13/100\n",
      "297/297 [==============================] - 99s 333ms/step - loss: 0.4603 - accuracy: 0.8168 - val_loss: 0.9811 - val_accuracy: 0.6451 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    # class_weight=class_weights,  # Handle class imbalance\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952/3952 [==============================] - 29s 7ms/step\n",
      "Signal-level predictions shape: (3411,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def aggregate_predictions_mode(segment_predictions, segment_labels):\n",
    "    \"\"\"\n",
    "    Aggregate segment predictions to produce signal-level predictions using mode.\n",
    "\n",
    "    Args:\n",
    "    - segment_predictions: Array of shape (num_segments,), predicted class labels for each segment.\n",
    "    - segment_labels: Array of shape (num_segments,), signal indices for each segment.\n",
    "\n",
    "    Returns:\n",
    "    - signal_predictions: Array of shape (num_signals,), aggregated predictions (most frequent class).\n",
    "    \"\"\"\n",
    "    num_signals = np.max(segment_labels) + 1  # Number of unique signals\n",
    "    signal_predictions = []\n",
    "\n",
    "    for signal_idx in range(num_signals):\n",
    "        # Extract predictions for all segments of this signal\n",
    "        signal_segment_preds = segment_predictions[segment_labels == signal_idx]\n",
    "\n",
    "        # Compute the mode (most common class)\n",
    "        signal_pred = mode(signal_segment_preds, axis=None)[0]\n",
    "        signal_predictions.append(signal_pred)\n",
    "\n",
    "    return np.array(signal_predictions)\n",
    "\n",
    "# Example usage\n",
    "segment_predictions = model.predict(X_test_segments)\n",
    "\n",
    "# Step 1: Predict segment-level classes\n",
    "segment_class_predictions = np.argmax(segment_predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Step 2: Aggregate to signal-level predictions\n",
    "signal_predictions = aggregate_predictions_mode(segment_class_predictions, segment_labels_test)\n",
    "\n",
    "# Check results\n",
    "print(\"Signal-level predictions shape:\", signal_predictions.shape)  # Should match the number of signal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = np.vstack((np.arange(X_test_raw.shape[0]), signal_predictions)).T\n",
    "# Save as a CSV file\n",
    "np.savetxt(\"submission.csv\", submission_data, delimiter=\",\", header=\"id,y\", comments=\"\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3411, 2)\n"
     ]
    }
   ],
   "source": [
    "print(signal_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 3s 28ms/step\n",
      "Submission file saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "def create_submission(model, X_test, filename=\"submission.csv\"):\n",
    "    # Get predictions as probabilities\n",
    "    prob_preds = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    class_preds = np.argmax(prob_preds, axis=1)\n",
    "    \n",
    "    # Create an array with IDs and corresponding predictions\n",
    "    submission_data = np.vstack((np.arange(X_test.shape[0]), class_preds)).T\n",
    "    \n",
    "    # Save as a CSV file\n",
    "    np.savetxt(filename, submission_data, delimiter=\",\", header=\"id,y\", comments=\"\", fmt=\"%d\")\n",
    "    print(f\"Submission file saved as {filename}\")\n",
    "\n",
    "\n",
    "# Create submission\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "create_submission(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: 1\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detected: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
